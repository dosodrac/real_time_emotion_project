{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project Prototype_images.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kL6w07-Fav5d"
      },
      "source": [
        "# **Final year project - Prototype**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNBr5GRDa6-j"
      },
      "source": [
        "## Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXTcUy6HZTNq"
      },
      "source": [
        "For the first step to run this notebook, you will need to upload the API token that I have generated in Kaggle.\n",
        "\n",
        "Please, run the following block of code: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "7Zehrb_prRTt",
        "outputId": "4410a369-751d-4e19-c641-ddcb7f375cd2"
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1f5a7cef-b2b2-4675-aa1a-3e1449a93696\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1f5a7cef-b2b2-4675-aa1a-3e1449a93696\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"osodracd\",\"key\":\"33b1c744630e7b5e17c75e86a314d8c3\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y24z5SE8rseo"
      },
      "source": [
        "Next click on *Choose files*. This will prompt a upload window where you need to locate, select and upload the kaggle.json file that I have attached to the zip file. \n",
        "\n",
        "Once the file is visible in the folder tree on the left side, you can follow to the next block of code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0RB9Ar5d5dh"
      },
      "source": [
        "## The dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQHDU-THeApw"
      },
      "source": [
        "The data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image.\n",
        "\n",
        "The task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87N0V7XAm3K4"
      },
      "source": [
        "# Kaggle installation\n",
        "! pip install kaggle\n",
        "# Create Kaggle directory\n",
        "! mkdir ~/.kaggle\n",
        "# Copy the kaggle.json file into the current directory\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "# provide owner full read and write access to the file\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# Download the dataset zip file\n",
        "! kaggle datasets download msambare/fer2013\n",
        "# Extract the files withtin the zip file\n",
        "! unzip fer2013.zip\n",
        "print('Unzip completed.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn7Ak8JNcQUq"
      },
      "source": [
        "Now that the dataset is downloaded and accessible, I will import all the necessary libraries and modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMtvS7yc2SaJ"
      },
      "source": [
        "# Import statements\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.preprocessing import image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras_preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Dense, Input, Dropout, GlobalAveragePooling2D, Flatten, Conv2D, BatchNormalization,Activation, MaxPooling2D\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import adam_v2, rmsprop_v2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKnRWJlxcv4N"
      },
      "source": [
        "## Preparing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw6eNbtA89KC"
      },
      "source": [
        "# Standardized size of images\n",
        "picture_size = 48\n",
        "folder_path = \"/content/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWv6Zo_D93uG"
      },
      "source": [
        "# Plotting neutral images from the folder\n",
        "expression = \"sad\"\n",
        "# Defining mapplotlib graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "# Plotting first 9 images\n",
        "for i in range (1, 10, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    img = load_img(folder_path+\"train/\"+expression+\"/\" + os.listdir(folder_path+\"train/\"+expression)[i],target_size= (picture_size, picture_size))\n",
        "    plt.imshow(img)\n",
        "plt.show()\n",
        "print('Example of ' + expression + ' faces.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwMAxW52_-K8"
      },
      "source": [
        "# Test plotting for a different expression\n",
        "expression = \"happy\"\n",
        "# Defining mapplotlib graph\n",
        "plt.figure(figsize=(12, 12))\n",
        "# Plotting first 9 images\n",
        "for i in range (1, 10, 1):\n",
        "    plt.subplot(3, 3, i)\n",
        "    img = load_img(folder_path+\"train/\"+expression+\"/\" + os.listdir(folder_path+\"train/\"+expression)[i],target_size= (picture_size, picture_size))\n",
        "    plt.imshow(img)\n",
        "plt.show()\n",
        "print('Example of ' + expression + ' faces.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rI4glsS-CKcM"
      },
      "source": [
        "## Training and validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeS-KUfqB2pj",
        "outputId": "f72cdc6d-fff4-44f8-8bee-c6cfa1bd50cf"
      },
      "source": [
        "# Training examples used per iteration\n",
        "batch_size = 128\n",
        "\n",
        "# Defining variables as image data generator\n",
        "datagenerator_train = ImageDataGenerator()\n",
        "datagenerator_val = ImageDataGenerator()\n",
        "\n",
        "# Variables containers of the data from each directory\n",
        "## Will collect all the images present in the directory and stored them at the variable\n",
        "train_set = datagenerator_train.flow_from_directory(folder_path + \"train\",\n",
        "                                                    target_size = (picture_size, picture_size),\n",
        "                                                    color_mode = \"grayscale\",\n",
        "                                                    batch_size = batch_size,\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                    shuffle = True)\n",
        "\n",
        "# Variables containers of the data from each directory\n",
        "## Will collect all the images present in the directory and stored them at the variable\n",
        "test_set = datagenerator_train.flow_from_directory(folder_path+\"test\",\n",
        "                                                    target_size = (picture_size, picture_size),\n",
        "                                                    color_mode = \"grayscale\",\n",
        "                                                    batch_size = batch_size,\n",
        "                                                    class_mode = \"categorical\",\n",
        "                                                    shuffle = True)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1pckFO3QFZA"
      },
      "source": [
        "## Initial Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFnESCOndFtI"
      },
      "source": [
        "In this section I will build the initial model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qh_4HXnwQEv3",
        "outputId": "b9f25352-1307-466a-9d11-566c30a8b1f1"
      },
      "source": [
        "number_of_classes = 7\n",
        "\n",
        "model = Sequential()\n",
        "# CNN layer 1\n",
        "model.add(Conv2D(32,(3,3), padding='same', input_shape = (48, 48, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25)) # Prevention of overfitting. Drops 25% of the data, setting those datapoints to 0. Generalize better and reach higher accurarcy\n",
        "\n",
        "# CNN layer 2\n",
        "model.add(Conv2D(64,(3,3), padding='same', input_shape = (48, 48, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25)) # Prevention of overfitting\n",
        "\n",
        "# CNN layer 3\n",
        "model.add(Conv2D(128,(3,3), padding='same', input_shape = (48, 48, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25)) # Prevention of overfitting\n",
        "\n",
        "# CNN layer 4\n",
        "model.add(Conv2D(512,(3,3), padding='same', input_shape = (48, 48, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.25)) # Prevention of overfitting\n",
        "\n",
        "model.add(Flatten()) # Collapse input size to one dimensional array\n",
        "\n",
        "# Fully connected layer 1\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Fully connected layer 2\n",
        "model.add(Dense(256))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(number_of_classes, activation='softmax'))\n",
        "\n",
        "opt = Adam (learning_rate=0.0001)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics = ['accuracy'])\n",
        "model.summary()\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_12 (Conv2D)          (None, 48, 48, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 48, 48, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 48, 48, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_12 (MaxPoolin  (None, 24, 24, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_18 (Dropout)        (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " conv2d_13 (Conv2D)          (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 24, 24, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_13 (MaxPoolin  (None, 12, 12, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_19 (Dropout)        (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " conv2d_14 (Conv2D)          (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " batch_normalization_20 (Bat  (None, 12, 12, 128)      512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_20 (Activation)  (None, 12, 12, 128)       0         \n",
            "                                                                 \n",
            " max_pooling2d_14 (MaxPoolin  (None, 6, 6, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_20 (Dropout)        (None, 6, 6, 128)         0         \n",
            "                                                                 \n",
            " conv2d_15 (Conv2D)          (None, 6, 6, 512)         590336    \n",
            "                                                                 \n",
            " batch_normalization_21 (Bat  (None, 6, 6, 512)        2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_21 (Activation)  (None, 6, 6, 512)         0         \n",
            "                                                                 \n",
            " max_pooling2d_15 (MaxPoolin  (None, 3, 3, 512)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_21 (Dropout)        (None, 3, 3, 512)         0         \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 256)               1179904   \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_22 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 256)               65792     \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 256)              1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_23 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 7)                 1799      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,935,495\n",
            "Trainable params: 1,932,999\n",
            "Non-trainable params: 2,496\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZjHIrLoUW7S",
        "outputId": "d03d77fd-fa75-428c-cd61-3f5e3de3c217"
      },
      "source": [
        "epochs = 10\n",
        "history = model.fit_generator (generator=train_set,\n",
        "                               steps_per_epoch=train_set.n//train_set.batch_size, # floor division of train set by batch size\n",
        "                               epochs = epochs,\n",
        "                               validation_data = test_set,\n",
        "                               validation_steps = test_set.n//test_set.batch_size)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "224/224 [==============================] - 335s 1s/step - loss: 1.6545 - accuracy: 0.3572 - val_loss: 1.6251 - val_accuracy: 0.3790\n",
            "Epoch 2/10\n",
            "224/224 [==============================] - 338s 2s/step - loss: 1.6140 - accuracy: 0.3756 - val_loss: 1.6132 - val_accuracy: 0.3945\n",
            "Epoch 3/10\n",
            "224/224 [==============================] - 339s 2s/step - loss: 1.5812 - accuracy: 0.3878 - val_loss: 1.5828 - val_accuracy: 0.4008\n",
            "Epoch 4/10\n",
            "224/224 [==============================] - 337s 2s/step - loss: 1.5478 - accuracy: 0.4028 - val_loss: 1.5618 - val_accuracy: 0.4051\n",
            "Epoch 5/10\n",
            "224/224 [==============================] - 335s 1s/step - loss: 1.5159 - accuracy: 0.4134 - val_loss: 1.5117 - val_accuracy: 0.4261\n",
            "Epoch 6/10\n",
            "224/224 [==============================] - 335s 1s/step - loss: 1.4886 - accuracy: 0.4251 - val_loss: 1.5034 - val_accuracy: 0.4340\n",
            "Epoch 7/10\n",
            "224/224 [==============================] - 338s 2s/step - loss: 1.4563 - accuracy: 0.4385 - val_loss: 1.4773 - val_accuracy: 0.4443\n",
            "Epoch 8/10\n",
            "224/224 [==============================] - 341s 2s/step - loss: 1.4399 - accuracy: 0.4459 - val_loss: 1.3915 - val_accuracy: 0.4704\n",
            "Epoch 9/10\n",
            "224/224 [==============================] - 337s 2s/step - loss: 1.4164 - accuracy: 0.4550 - val_loss: 1.3582 - val_accuracy: 0.4780\n",
            "Epoch 10/10\n",
            "224/224 [==============================] - 336s 2s/step - loss: 1.3886 - accuracy: 0.4659 - val_loss: 1.3321 - val_accuracy: 0.4863\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LsSOuHAdrii"
      },
      "source": [
        "## Conclusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skMvGFDPe1Qu"
      },
      "source": [
        "For this project specifications I have set the aim to achieve 66% of effectiveness, in line similar studies. \n",
        "\n",
        "Overall, this prototype would have a baseline of approximately 15% accuracy for a balanced dataset, taking that 100% accuracy would be divided by the 7 classes. The current results prove that this prototype has statistical power for it has achieved validation accuracy higher than 15%, at 47+%, hence providing the required proof of concept. \n",
        "\n",
        "Moving forward, I will improve the model through the tuning of hyperparameters such as the optimizer, the learning rate, the number of layers, the number of units per layer and the momentum. I will also fight overfitting through the application of measures such as adding more data, network size reduction and weight regularization.\n",
        "\n",
        "Once the deep learning model has achieved accuracy close to the minimum viable product (MVP), I will expand it into the final project by importing module CV2 and connect the notebook to a live webcam with the purpose of feeding unseen data, ultimately testing the effectiveness and accomplishing the planned Convolutional Neural Networks for Emotion Classification. Then, I will develop the research aspect of the project through the execution of the proposed user testing, culminating in a comparation and evaluation study.\n",
        "\n",
        "I am confident that this prototype implements all the functionality of the project to an initially working standard, being very clear that the project is doable within the timeframe of this module, for is already close to the MVP, which makes me believe that is possible to produce a complete and finished product as idealized initially."
      ]
    }
  ]
}